# План Экспериментов: Сравнительное Исследование Архитектур Трансформеров

## Общая Цель

Разработать и описать план экспериментов для всестороннего изучения и сравнения трех основных архитектур трансформеров (encoder-only, decoder-only, sequence-to-sequence) на задачах обработки данных, подходящих для каждой архитектуры. Цель — оценить их производительность (качество решения задачи) и эффективность (ресурсоемкость).

## Общие Требования и Инструменты

* **Библиотеки:** Использовать экосистему Hugging Face:
    * `transformers` для загрузки моделей и токенизаторов.
    * `datasets` для загрузки и предобработки данных.
    * `evaluate` для расчета стандартных метрик.
* **Оборудование:** Эксперименты рассчитаны на выполнение с использованием GPU Nvidia A40 (48 ГБ VRAM). Необходимо учитывать это при выборе batch size и использовать смешанную точность (`fp16=True`) для ускорения и экономии памяти.Так же я использую SLURM технологию, так как работаю на сервере общего пользования.
* **Мониторинг:** Отслеживать потребление памяти GPU (например, с помощью `GPUtil` или `nvidia-smi`) и время выполнения (обучения и инференса).
* **Фокус Сравнения:** Основное внимание уделить сравнению моделей внутри каждой архитектуры и между архитектурами по метрикам производительности и эффективности. Также оценить влияние изменения топологии (уменьшения числа слоев) и дообучения на дополнительном датасете.

---

## 1. Encoder-only Модели: Бинарная Классификация Тональности

* **Задача:** Бинарная классификация тональности текстов (позитивный/негативный). Подходит для encoder-only моделей, так как требует глубокого понимания контекста всего входного текста.
* **Основной Датасет:** IMDb (`imdb`) – стандартный бенчмарк для классификации отзывов.
* **Модели для Сравнения:**
    1.  `bert-base-uncased` (110M параметров)
    2.  `roberta-base` (125M параметров)
    3.  `distilbert-base-uncased` (66M параметров, легкая версия BERT)
* **Эксперименты:**
    1.  **Baseline Fine-tuning:**
        * Тонкая настройка каждой модели на датасете IMDb.
        * **Гиперпараметры (примерные):** batch_size=16, epochs=3, learning_rate=5e-5, fp16=True.
        * **Цель:** Установить базовую производительность и ресурсоемкость каждой модели на задаче.
    2.  **Изменение Топологии:**
        * Взять модель `roberta-base` и уменьшить количество слоев с 12 до 6 (`num_hidden_layers=6`).
        * Повторить fine-tuning на IMDb с теми же гиперпараметрами.
        * **Цель:** Оценить влияние уменьшения сложности модели на производительность и эффективность.
    3.  **Дообучение (Transfer Learning):**
        * Взять лучшую модель (например, `roberta-base`), дообученную на IMDb (из эксперимента 1).
        * Продолжить её обучение на другом датасете схожей тематики – Yelp Polarity (`yelp_polarity`).
        * **Гиперпараметры (примерные):** learning_rate=2e-5, epochs=1-2.
        * **Цель:** Оценить, улучшает ли дообучение на смежном датасете метрики на исходном датасете (IMDb) или на новом.
* **Метрики:**
    * **Производительность:** Accuracy, F1-score (weighted).
    * **Эффективность:** Время инференса (мс/пример), Пиковое потребление памяти GPU (ГБ) во время обучения/инференса, Общее время обучения (секунды).

---

## 2. Decoder-only Модели: Условная Генерация Заголовков

* **Задача:** Условная генерация текста – создание короткого заголовка на основе текста новостной статьи. Подходит для decoder-only моделей, так как они оптимизированы для авторегрессивной генерации последовательностей.
* **Основной Датасет:** XSum (`xsum`) – статьи с очень краткими, абстрактными заголовками.
* **Модели для Сравнения:**
    1.  `gpt2` (GPT-2-base, 117M параметров)
    2.  `distilgpt2` (82M параметров, легкая версия GPT-2)
* **Эксперименты:**
    1.  **Baseline Fine-tuning:**
        * Тонкая настройка каждой модели на датасете XSum для генерации заголовка по статье (формат "Article: [text] Summary: [headline]").
        * **Гиперпараметры (примерные):** batch_size=8 (требуют больше памяти), epochs=3, learning_rate=5e-5, fp16=True.
        * **Цель:** Установить базовое качество генерации и ресурсоемкость.
    2.  **Изменение Топологии:**
        * Взять модель `gpt2` и уменьшить количество слоев с 12 до 6 (`n_layer=6`).
        * Повторить fine-tuning на XSum.
        * **Цель:** Оценить влияние упрощения модели на качество генерации и эффективность.
    3.  **Дообучение (Transfer Learning):**
        * Взять модель `gpt2`, дообученную на XSum.
        * Продолжить её обучение на другом датасете для генерации заголовков/суммаризации – BBC News Summary (`g8a9/bbc_news_summary`).
        * **Гиперпараметры (примерные):** learning_rate=2e-5, epochs=1-2.
        * **Цель:** Оценить перенос знаний и улучшение метрик на исходном (XSum) или новом датасете.
* **Метрики:**
    * **Производительность:** ROUGE (ROUGE-1, ROUGE-2, ROUGE-L), BLEU.
    * **Эффективность:** Время инференса (мс/пример), Пиковое потребление памяти GPU (ГБ), Общее время обучения (секунды).

---

## 3. Sequence-to-Sequence Модели: Суммаризация Текстов

* **Задача:** Абстрактивная суммаризация – создание краткого содержания (summary/highlights) на основе длинной новостной статьи. Идеально подходит для seq2seq моделей, имеющих энкодер для понимания входа и декодер для генерации выхода.
* **Основной Датасет:** CNN/DailyMail (`cnn_dailymail`, версия `3.0.0`) – статьи с несколькими пунктами содержания.
* **Модели для Сравнения:**
    1.  `t5-base` (220M параметров)
    2.  `facebook/bart-base` (139M параметров)
* **Эксперименты:**
    1.  **Baseline Fine-tuning:**
        * Тонкая настройка каждой модели на датасете CNN/DailyMail. Для T5 использовать префикс "summarize: ".
        * **Гиперпараметры (примерные):** batch_size=8, epochs=3, learning_rate=5e-5, fp16=True.
        * **Цель:** Установить базовое качество суммаризации и ресурсоемкость.
    2.  **Изменение Топологии:**
        * Взять модель `t5-base` и уменьшить количество слоев энкодера и декодера с 12 до 6 (`num_layers=6`, `num_decoder_layers=6`).
        * Повторить fine-tuning на CNN/DailyMail.
        * **Цель:** Оценить влияние упрощения модели на качество суммаризации и эффективность.
    3.  **Дообучение (Transfer Learning):**
        * Взять модель `t5-base`, дообученную на CNN/DailyMail.
        * Продолжить её обучение на датасете XSum (который использовался для decoder-only), так как он тоже является датасетом для суммаризации/генерации заголовков.
        * **Гиперпараметры (примерные):** learning_rate=2e-5, epochs=1-2.
        * **Цель:** Оценить адаптивность модели к другому стилю суммаризации и возможное улучшение метрик.
* **Метрики:**
    * **Производительность:** ROUGE (ROUGE-1, ROUGE-2, ROUGE-L), BLEU.
    * **Эффективность:** Время инференса (мс/пример), Пиковое потребление памяти GPU (ГБ), Общее время обучения (секунды).

---

## Ожидаемый Результат

Подробное описание плана экспериментов, включая:
* Четко определенные задачи для каждого типа архитектуры.
* Выбранные модели и датасеты с обоснованием.
* Описанные шаги экспериментов (baseline, изменение топологии, дообучение).
* Перечень метрик для оценки производительности и эффективности.
* Структурированные таблицы для записи и сравнения результатов по всем экспериментам и моделям (можно оставить пустыми как шаблон).
* Анализ ожидаемых результатов и компромиссов (trade-offs) между производительностью и эффективностью для разных моделей и настроек.
### Ответ на ваш запрос

Вы можете протестировать три типа трансформеров с нуля:
1. **Encoder-only** (BERT, RoBERTa, DistilBERT) на **бинарной классификации тональности** (IMDb), с экспериментами по baseline, уменьшению слоев и дообучению на Yelp Polarity.
2. **Decoder-only** (GPT-2, DistilGPT2) на **генерации заголовков** (XSum), с аналогичными экспериментами и дообучением на BBC News Summary.
3. **Sequence-to-sequence** (T5, BART) на **суммаризации текстов** (CNN/DailyMail), с дообучением на XSum.

Этот план:
- Охватывает все типы трансформеров.
- Соответствует вашей теме («Обработка данных»).
- Использует Hugging Face и Nvidia A40 GPU.
- Позволяет сравнить производительность, эффективность и влияние топологических изменений.

Если вам нужен конкретный код, помощь с настройкой экспериментов или структурированием результатов для работы, дайте знать!